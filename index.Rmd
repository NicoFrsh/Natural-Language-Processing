---
title: "Milestone Report 1"
author: "Nico Frisch"
date: "26 8 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Milestone Report

I decided to use the quanteda package for all of my data preprocessing and exploratory analysis.

```{r}
library(readtext)
library(quanteda)

all_data <- readtext(paste0(getwd(), "/Samples/*.txt"), encoding = "UTF-8")
```

Next, we create a quanteda corpus object. As there were some encoding problems I had to substitute all non-UTF-8 encoded characters to ASCII.

```{r, echo=TRUE, cache=TRUE}
# create a corpus 
corp <- corpus(all_data)

# remove non utf-8 characters
texts(corp) <- iconv(texts(corp), from = "UTF-8", to = "ASCII", sub = "")
```

Now, we can tokenize our corpus object. We want to have word tokens and remove all numbers, punctuation, symbols and URLs.

```{r, cache=TRUE}
tok <- tokens(corp, what = "fasterword", remove_punct = TRUE, remove_symbols = TRUE,
              remove_numbers = TRUE, remove_url = TRUE, split_hyphens = TRUE)
```

As another part of the data cleaning process we want to remove english stopwords like "the", "a", "is", etc. as they should not be taken into account for our further data analysis. Also, we want all our words to be in lower case and lastly we want to remove profanity words. For the latter case I used [this](https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/3b70dd644cec678ddc43da88d30034add22897ef/google_twunter_lol) list from Github.

```{r}
# remove stop words
tok <- tokens_select(tok, stopwords('english'),selection='remove')

# to lower case
tok <- tokens_tolower(tok)

# remove profanity
con <- file("https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/3b70dd644cec678ddc43da88d30034add22897ef/google_twunter_lol", "r")
profList <- readLines(con)
close(con)

tok <- tokens_select(tok, profList, selection = "remove")
```

Although there are still some issues about the texts, such as unwanted dots or commas that were not properly removed by the remove_punct function, we concede to the next step of our analysis, the exploratory analysis part.

## Exploratory Analysis

First, we want to get an overview on the distribution of word frequencies. To get the most frequent words we have to create a Document Feature Matrix (DFM), then we can use the topfeatures function of the quanteda package.

```{r}
dfm <- dfm(tok)

# top featured words
top20 <- topfeatures(dfm, 20)

freq <- data.frame(feature = names(top20), frequency = top20)

library(ggplot2)
gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) + 
        geom_bar(stat = "identity") +
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Features")

gg
```

Another cool feature to visualize the distribution of word frequencies is the textplot_wordcloud function:

```{r}
textplot_wordcloud(dfm, random_order = FALSE, max_words = 150, rotation = 0.25,
                   color = RColorBrewer::brewer.pal(8, "Dark2"))
```

N-grams are word chains of N words that appear in our corpus. We want to know the frequencies of 2-grams and 3-grams in our dataset.

```{r}
# create n-grams
biGram <- tokens_ngrams(tok, n = 2)
triGram <- tokens_ngrams(tok, n = 3)

# get frequencies and plot
top20bigram <- topfeatures(dfm(biGram), 20)
freq <- data.frame(feature = names(top20bigram), frequency = top20bigram)

gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_bar(stat = "identity") + 
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Bi-Grams")

gg

top20trigram <- topfeatures(dfm(triGram), 20)
freq <- data.frame(feature = names(top20trigram), frequency = top20trigram)

gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_bar(stat = "identity") + 
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Tri-Grams")

gg
```

This already looks quite nice, there are mostly bi- and tri-grams that make actually sense.
Nonetheless, there appears the bi-gram 'said._"i' several times.

Next, we want to anser the question how many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? What about 90%? To find that out, we construct a new dataframe containing the by frequency sorted words of our data, their cumulative sum and proportion of the total amount of words in all of our text data. The plot below shows the coverage (y-axis) along the number of sorted unique words (x-axis). The red line shows the point at which the coverage surpasses 50% - needing the first 1818 most frequent words - and the blue line where it surpasses 90%, achieved by including the first 18554 words.  

```{r}
topfeat <- topfeatures(dfm, length(dfm))
cvrg <- data.frame(n = 1:length(topfeat), total = cumsum(topfeat))
cvrg$prop <- cvrg$total / cvrg$total[length(topfeat)]

plot(cvrg$n, cvrg$prop, type = "l", xlab = "Number of unique words", 
     ylab = "Proportion of total words", main = "Coverage by frequency sorted dictionary")
# find first word that surpasses the 50% mark
ind50 <- which(cvrg$prop > 0.5)[1]
# now for the 90%
ind90 <- which(cvrg$prop > 0.9)[1]
abline(v = ind50, col = "red")
abline(v = ind90, col = "blue")
ind50
ind90
```

Another question we should ask ourselves is how to increase the coverage, i.e. identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases? One solution would be to trim our data set, so that we delete words that only appear very few times. Those words may be very specific and thus not very helpful for our further modelling.

```{r}
# check number of words in each document before trimming
ntoken(dfm)
dfm <- dfm_trim(dfm, min_termfreq = 5, termfreq_type = "count")
# check number of words afterwards
ntoken(dfm)
```

Doing so, we removed some reasonable amount of words from our data.