---
title: "Milestone Report 1"
author: "Nico Frisch"
date: "26 8 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Milestone Report

About the data ....
In order to save computation time I sampled 10% of the data of all texts and saved those samples in separate text files. These smaller samples are stored in a folder named 'Samples' and can be loaded completely starting the analysis.
I decided to use the quanteda package for all of my data preprocessing and exploratory analysis. To gather the data from the 3 text files I used the readtext function from the equally named package. 

```{r}
library(readtext)
library(quanteda)

all_data <- readtext(paste0(getwd(), "/Samples/*.txt"), encoding = "UTF-8")
```

Next, we create a quanteda corpus object. As there were some encoding problems I had to substitute all non-UTF-8 encoded characters to ASCII.

```{r, cache=TRUE}
# create a corpus 
corp <- corpus(all_data)

# remove non utf-8 characters
texts(corp) <- iconv(texts(corp), from = "UTF-8", to = "ASCII", sub = "")
texts(corp) <- gsub("[\u2019\u0092]","'",texts(corp))
# remove punctuation
texts(corp) <- gsub('[\\,\"\\.\\*\\?!#:/();]', "", texts(corp))
```

Now, we can tokenize our corpus object. We want to have word tokens and remove all numbers, punctuation, symbols and URLs.

```{r, cache=TRUE}
tok <- tokens(corp, what = "fasterword", remove_punct = TRUE, remove_symbols = TRUE,
              remove_numbers = TRUE, remove_url = TRUE, split_hyphens = TRUE)
```

As another part of the data cleaning process we want to remove profanity words. In this case I used [this](https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/3b70dd644cec678ddc43da88d30034add22897ef/google_twunter_lol) list from Github.

```{r}
# to lower case
tok <- tokens_tolower(tok)

# remove profanity
con <- file("https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/3b70dd644cec678ddc43da88d30034add22897ef/google_twunter_lol", "r")
profList <- readLines(con)
close(con)

tok <- tokens_select(tok, profList, selection = "remove")
```

Although there are still some issues about the texts, such as unwanted dots or commas that were not properly removed by the remove_punct function, we concede to the next step of our analysis, the exploratory analysis.

## Exploratory Analysis

First, we want to get an overview on the distribution of word frequencies. To get the most frequent words we have to create a Document Feature Matrix (DFM), then we can use the topfeatures function of the quanteda package. The DFM contains for each document in our corpus every 'feature' - in our case every word - together with its count value.

```{r}
dfm <- dfm(tok)

# top featured words
top20 <- topfeatures(dfm, 20)

freq <- data.frame(feature = names(top20), frequency = top20)

library(ggplot2)
gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) + 
        geom_bar(stat = "identity") +
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Features")

gg
```

Another cool feature to visualize the distribution of word frequencies is the textplot_wordcloud function:

```{r}
textplot_wordcloud(dfm, random_order = FALSE, max_words = 150, rotation = 0.25,
                   color = RColorBrewer::brewer.pal(8, "Dark2"))
```

N-grams are word chains of N words that appear in our corpus. We want to know the frequencies of 2-grams and 3-grams in our dataset.

```{r}
# create n-grams
biGram <- tokens_ngrams(tok, n = 2, concatenator = " ")
triGram <- tokens_ngrams(tok, n = 3, concatenator = " ")

# get frequencies and plot
top20bigram <- topfeatures(dfm(biGram), 20)
freq <- data.frame(feature = names(top20bigram), frequency = top20bigram)

gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_bar(stat = "identity") + 
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Bi-Grams")

gg

top20trigram <- topfeatures(dfm(triGram), 20)
freq <- data.frame(feature = names(top20trigram), frequency = top20trigram)

gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_bar(stat = "identity") + 
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Tri-Grams")

gg
```

This already looks quite nice, there are mostly bi- and tri-grams that make actually sense.

Next, we want to answer the question how many unique words we need in a frequency sorted dictionary to cover 50% and 90% of all word instances in the language. To find that out, we construct a new dataframe containing the by frequency sorted words of our data, their cumulative sum and proportion of the total amount of words in all of our text data. The plot below shows the coverage (y-axis) along the number of sorted unique words (x-axis). The red line shows the point at which the coverage surpasses 50% - requiring only the first 143 most frequent words - and the blue line where it surpasses 90%, achieved by including the first 6730 words.  

```{r}
topfeat <- topfeatures(dfm, length(dfm))
cvrg <- data.frame(n = 1:length(topfeat), total = cumsum(topfeat))
cvrg$prop <- cvrg$total / cvrg$total[length(topfeat)]

plot(cvrg$n, cvrg$prop, type = "l", xlab = "Number of unique words", 
     ylab = "Proportion of total words", main = "Coverage by frequency sorted dictionary")
# find first word that surpasses the 50% mark
ind50 <- which(cvrg$prop > 0.5)[1]
# now for the 90%
ind90 <- which(cvrg$prop > 0.9)[1]
abline(v = ind50, col = "red")
abline(v = ind90, col = "blue")
ind50
ind90
```

Now, we want to check for words from foreign languages. I used [this](https://raw.githubusercontent.com/dwyl/english-words/master/words.txt) dictionary of english words to find all foreign words in the data set.

```{r, cache=TRUE}
# Find words from foreign languages.
# use english dictionary dataset
url <- "https://raw.githubusercontent.com/dwyl/english-words/master/words.txt"
cnct <- file(url, "r")
enDict <- readLines(cnct)
close(cnct)

foreignWords <- tokens_select(tok, enDict, selection = "remove")
foreignWords
ntoken(foreignWords)
sum(ntoken(foreignWords))
```

So there are 360600 foreign words in total in our data. Lets remove those foreign words:

```{r, cache=TRUE}
# remove foreign words
sum(ntoken(tok))
tok <- tokens_select(tok, enDict, selection = "keep")
sum(ntoken(tok))
```

Another question we should ask ourselves is how to increase the coverage, i.e. identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases? One solution would be to trim our data set by deleting words that only appear very few times. Those words may be very specific and thus not very helpful for our further modelling.

```{r}
# check number of words in each document before trimming
ntoken(dfm)
dfm <- dfm_trim(dfm, min_termfreq = 5, termfreq_type = "count")
# check number of words afterwards
ntoken(dfm)
```

Doing so, we removed some reasonable amount of words from our data.

## Modeling

Naive approach:

```{r}
# naive approach
predictWord <- function(toks, prevWords){
        word_bigram <- tokens_compound(toks, pattern = phrase(paste0(prevWords, " *")))
        prevWords <- stringr::str_replace(prevWords, " ", "_")
        word_bigram_select <- tokens_select(word_bigram, pattern = phrase(paste0(prevWords, "_*")))
        
        topfeatures(dfm(word_bigram_select), 3)
}

predictWord(tok, "just")
predictWord(tok, "i")
predictWord(tok, "why")
predictWord(tok, "do you")
```