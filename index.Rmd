---
title: "Milestone Report 1"
author: "Nico Frisch"
date: "26 8 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## About the Data

The goal of this project is to implement a Natural Language Processing prediction algorithm to predict the next word given the previous 1, 2 or 3 words. Those prrediction models are widely used, i.e. on Mobile Devices and SwiftKey is one of the pioneers in this area. For this project we are using the HC Corpora which contains text data from blogs, news and twitter documents in serveral languages. We will focus on the english data sets. The raw data has a total of **over 4 million lines** in all three documents and **over 100 million words**. That's **over half a Gigabyte of data**.


In order to save computation time I sampled 10% of the data of all texts and saved those samples in separate text files. These smaller samples are stored in a folder named 'Samples' and can be loaded completely starting the analysis. By sampling I was able to reduce the data to **427000 lines**, a bit over **10 million words** and a total of about **55 Megabytes**.

## Getting and Cleaning Data

I decided to use the quanteda package for all of my data preprocessing and exploratory analysis. To gather the data from the 3 text files I used the readtext function from the equally named package. 

```{r}
library(readtext)
library(quanteda)

all_data <- readtext(paste0(getwd(), "/Samples/*.txt"), encoding = "UTF-8")
```

Next, we create a quanteda corpus object. As there were some encoding problems I had to substitute all non-UTF-8 encoded characters to ASCII.

```{r, cache=TRUE}
# create a corpus 
corp <- corpus(all_data)

# remove non utf-8 characters
texts(corp) <- iconv(texts(corp), from = "UTF-8", to = "ASCII", sub = "")
texts(corp) <- gsub("[\u2019\u0092]","'",texts(corp))
# remove punctuation
texts(corp) <- gsub('[\\,\"\\.\\*\\?!#:/();]', "", texts(corp))
```

Now, we can tokenize our corpus object. We want to have word tokens and remove all numbers, punctuation, symbols and URLs.

```{r, cache=TRUE}
tok <- tokens(corp, what = "fasterword", remove_punct = TRUE, remove_symbols = TRUE,
              remove_numbers = TRUE, remove_url = TRUE, split_hyphens = TRUE)
```

As another part of the data cleaning process we want to remove profanity words. In this case I used [this](https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/3b70dd644cec678ddc43da88d30034add22897ef/google_twunter_lol) list from Github.

```{r}
# to lower case
tok <- tokens_tolower(tok)

# remove profanity
con <- file("https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/3b70dd644cec678ddc43da88d30034add22897ef/google_twunter_lol", "r")
profList <- readLines(con)
close(con)

tok <- tokens_select(tok, profList, selection = "remove")
```

I consciously decided **not** to remove stop words such as "the", "a", "i", "and", etc. as I want them to be included in the analysis and later prediction algorithm.

## Exploratory Analysis

First, we want to get an overview on the distribution of word frequencies. To get the most frequent words we have to create a Document Feature Matrix (DFM), then we can use the topfeatures function of the quanteda package. The DFM contains for each document in our corpus every 'feature' - in our case every word - together with its count value.

```{r}
dfm <- dfm(tok)

# top featured words
top20 <- topfeatures(dfm, 20)

freq <- data.frame(feature = names(top20), frequency = top20)

library(ggplot2)
gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) + 
        geom_bar(stat = "identity") +
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Features")

gg
```

Another cool feature to visualize the distribution of word frequencies is the textplot_wordcloud function:

```{r}
textplot_wordcloud(dfm, random_order = FALSE, max_words = 150, rotation = 0.25,
                   color = RColorBrewer::brewer.pal(8, "Dark2"))
```

N-grams are word chains of N words that appear in our corpus. We want to know the frequencies of 2-grams and 3-grams in our dataset.

```{r}
# create n-grams
biGram <- tokens_ngrams(tok, n = 2, concatenator = " ")
triGram <- tokens_ngrams(tok, n = 3, concatenator = " ")

# get frequencies and plot
top20bigram <- topfeatures(dfm(biGram), 20)
freq <- data.frame(feature = names(top20bigram), frequency = top20bigram)

gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_bar(stat = "identity") + 
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Bi-Grams")

gg

top20trigram <- topfeatures(dfm(triGram), 20)
freq <- data.frame(feature = names(top20trigram), frequency = top20trigram)

gg <- ggplot(freq, aes(x = reorder(feature, frequency), y = frequency)) +
        geom_bar(stat = "identity") + 
        coord_flip() +
        labs(x = NULL, y = "Frequency", title = "Top 20 Tri-Grams")

gg
```

This already looks quite nice, all of the most frequent bi- and tri-grams make actually sense.

Next, we want to answer the question how many unique words we need in a frequency sorted dictionary to cover 50% and 90% of all word instances in the language. To find that out, we construct a new dataframe containing the by frequency sorted words of our data, their cumulative sum and proportion of the total amount of words in all of our text data. The plot below shows the coverage (y-axis) along the number of sorted unique words (x-axis). The red line shows the point at which the coverage surpasses 50% - requiring only the first 143 most frequent words - and the blue line where it surpasses 90%, achieved by including the first 6730 words.  

```{r}
topfeat <- topfeatures(dfm, length(dfm))
cvrg <- data.frame(n = 1:length(topfeat), total = cumsum(topfeat))
cvrg$prop <- cvrg$total / cvrg$total[length(topfeat)]

plot(cvrg$n, cvrg$prop, type = "l", xlab = "Number of unique words", 
     ylab = "Proportion of total words", main = "Coverage by frequency sorted dictionary")
# find first word that surpasses the 50% mark
ind50 <- which(cvrg$prop > 0.5)[1]
# now for the 90%
ind90 <- which(cvrg$prop > 0.9)[1]
abline(v = ind50, col = "red")
abline(v = ind90, col = "blue")
ind50
ind90
```

Now, we want to check for words from foreign languages. I used [this](https://raw.githubusercontent.com/dwyl/english-words/master/words.txt) dictionary of english words to find all foreign words in the data set.

```{r, cache=TRUE}
# Find words from foreign languages.
# use english dictionary dataset
url <- "https://raw.githubusercontent.com/dwyl/english-words/master/words.txt"
cnct <- file(url, "r")
enDict <- readLines(cnct)
close(cnct)

foreignWords <- tokens_select(tok, enDict, selection = "remove")
foreignWords
ntoken(foreignWords)
sum(ntoken(foreignWords))
```

So there are 360600 foreign words in total in our data. Lets remove them:

```{r, cache=TRUE}
# remove foreign words
tok <- tokens_select(tok, enDict, selection = "keep")
```

Another question we should ask ourselves is how to increase the coverage, i.e. identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases? One solution would be to trim our data set by deleting words that only appear very few times. Those words may be very specific and thus not very helpful for our further modelling.

```{r}
# check number of words in each document before trimming
dfm <- dfm(tok)
ntoken(dfm)
dfm <- dfm_trim(dfm, min_termfreq = 5, termfreq_type = "count")
# check number of words afterwards
ntoken(dfm)
```

Doing so, we removed some reasonable amount of words from our data. After all, we have reduced our data from over 10 million to 9.6 million words and we have learned that we only need the 6730 most frequent words to cover 90% of the total word usage, which might be useful for our prediction algorithm later.  

## Modeling

A first naive prediction approach could be the following function. Its input arguments are the quanteda tokens object and a string containing the previous words we want to use to predict the following word. The function takes this string and finds all existing N-grams in the tokens object containing the string plus one more word. Then it finds the three most frequent N-grams and returns their last word, which are our three suggestions.

```{r}
# naive approach
predictWord <- function(toks, prevWords){
        word_ngram <- tokens_compound(toks, pattern = phrase(paste0(prevWords, " *")))
        prevWords <- stringr::str_replace(prevWords, " ", "_")
        word_ngram_select <- tokens_select(word_ngram, pattern = phrase(paste0(prevWords, "_*")))
        
        top3 <- topfeatures(dfm(word_ngram_select), 3)
        
        # filter only the following word
        sub(".*_", "", names(top3))
}

predictWord(tok, "just")
predictWord(tok, "i")
predictWord(tok, "why")
predictWord(tok, "do you")
```

The algorithm seems to work pretty well, all of the suggested words make sense.

Problems to consider:

- tokens object is still too large so that the computation time exceeds our expectations, so we have to reduce our training data even more, for example using only the 6730 words that have a coverage of 90%
- some N-grams are unseen in our training data, we need to find a way to predict the next word in those cases